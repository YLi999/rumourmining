{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import os\n",
    "\n",
    "rootPath = \"D:/Master/Thesis/Rumour Detection/Task/Train/semeval2017-task8-dataset/semeval2017-task8-dataset\"\n",
    "TweetsRootPath = rootPath + \"/\" + \"rumoureval-data\"\n",
    "TrainingDataPath = rootPath + \"/\" + \"traindev\"\n",
    "SubTaskATrainFile = \"rumoureval-subtaskA-train.json\"\n",
    "SubTaskADevFile = \"rumoureval-subtaskA-dev.json\"\n",
    "SubTaskADevResultsFile = \"rumoureval-subtaskA-dev-results.json\"\n",
    "SubTaskATestResultsFile = \"rumoureval-subtaskA-test-results.json\"\n",
    "SubTaskAGoldResultsFile = \"rumoureval-subtaskA-gold-results.json\"\n",
    "SubTaskATrainFilePath = os.path.join(TrainingDataPath,SubTaskATrainFile)\n",
    "SubTaskADevFilePath = os.path.join(TrainingDataPath,SubTaskADevFile)\n",
    "SubTaskADevResultsFilePath = os.path.join(TrainingDataPath,SubTaskADevResultsFile)\n",
    "SubTaskATestResultsFilePath = os.path.join(TrainingDataPath,SubTaskATestResultsFile)\n",
    "SubTaskAGoldResultsFilePath = os.path.join(TrainingDataPath,SubTaskAGoldResultsFile)\n",
    "\n",
    "SubTaskBTrainFile = \"rumoureval-subtaskB-train.json\"\n",
    "SubTaskBDevFile = \"rumoureval-subtaskB-dev.json\"\n",
    "SubTaskBDevResultsFile = \"rumoureval-subtaskB-dev-results.json\"\n",
    "SubTaskBTestResultsFile = \"rumoureval-subtaskB-test-results.json\"\n",
    "SubTaskBGoldResultsFile = \"rumoureval-subtaskB-gold-results.json\"\n",
    "SubTaskBTrainFilePath = os.path.join(TrainingDataPath,SubTaskBTrainFile)\n",
    "SubTaskBDevFilePath = os.path.join(TrainingDataPath,SubTaskBDevFile)\n",
    "SubTaskBDevResultsFilePath = os.path.join(TrainingDataPath,SubTaskBDevResultsFile)\n",
    "SubTaskBTestResultsFilePath = os.path.join(TrainingDataPath,SubTaskBTestResultsFile)\n",
    "SubTaskBGoldResultsFilePath = os.path.join(TrainingDataPath,SubTaskBGoldResultsFile)\n",
    "\n",
    "testRootPath = \"D:/Master/Thesis/Rumour Detection/Task/Test\"\n",
    "TestTweetsRootPath = testRootPath + \"/\" + \"rumoureval2017-test/semeval2017-task8-test-data\"\n",
    "\n",
    "#Gold Results data\n",
    "goldRootPath = \"D:/Master/Thesis/Rumour Detection/Task/Test/Gold\"\n",
    "SubTaskAGoldFile = \"subtaskA_gold.json\"\n",
    "SubTaskBGoldFile = \"subtaskB_gold.json\"\n",
    "SubTaskAGoldFilePath = goldRootPath + \"/\" + SubTaskAGoldFile\n",
    "SubTaskBGoldFilePath = goldRootPath + \"/\" + SubTaskBGoldFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open(SubTaskATrainFilePath) as data_file:    \n",
    "    TaskATrainData = json.load(data_file)\n",
    "\n",
    "with open(SubTaskADevFilePath) as data_file:    \n",
    "    TaskADevData = json.load(data_file)\n",
    "    \n",
    "with open(SubTaskBTrainFilePath) as data_file:    \n",
    "    TaskBTrainData = json.load(data_file)\n",
    "\n",
    "with open(SubTaskBDevFilePath) as data_file:    \n",
    "    TaskBDevData = json.load(data_file)\n",
    "    \n",
    "with open(SubTaskAGoldFilePath) as data_file:    \n",
    "    TaskAGoldData = json.load(data_file)\n",
    "    \n",
    "with open(SubTaskBGoldFilePath) as data_file:    \n",
    "    TaskBGoldData = json.load(data_file)\n",
    "#pprint(data)\n",
    "#TaskATrainData[\"544284128615473152\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetTweets(rootPath,TweetsFolderName):\n",
    "    TweetsJSON = { }\n",
    "    #print(\"rootPath = \" + rootPath)\n",
    "    for root, subFolders, files in os.walk(rootPath):\n",
    "        #print(\"root = \" + root)\n",
    "        for folder in subFolders:\n",
    "            if folder == TweetsFolderName:\n",
    "                srcTweetDir = os.path.join(root,folder)\n",
    "                #print(\"srcTweetDir = \" + srcTweetDir)\n",
    "                for  TweetRootPath, _,TweetFileNames in os.walk(srcTweetDir):\n",
    "                    for TweetFileName in TweetFileNames:\n",
    "                        filePath = os.path.join(TweetRootPath,TweetFileName)\n",
    "                        #print(\"filePath = \" + filePath)\n",
    "                        with open(filePath) as data_file:\n",
    "                            data = json.load(data_file)\n",
    "                            tweetID = TweetFileName[0:18]\n",
    "                            TweetsJSON[tweetID] = data\n",
    "    return TweetsJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SrcTweetsMap = GetTweets(TweetsRootPath,\"source-tweet\")\n",
    "ReplyTweetsMap = GetTweets(TweetsRootPath,\"replies\")\n",
    "#print(SrcTweetsMap[\"553164985460068352\"])\n",
    "\n",
    "#print(ReplyTweetsMap[\"581219554912800768\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SrcTestTweetsMap =  GetTweets(TestTweetsRootPath,\"source-tweet\")\n",
    "ReplyTestTweetsMap = GetTweets(TestTweetsRootPath,\"replies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetTweetObj(ID,IsTest):\n",
    "    tweetObj = None\n",
    "    \n",
    "    if IsTest:\n",
    "        if ID in ReplyTestTweetsMap:\n",
    "            tweetObj = ReplyTestTweetsMap[ID].copy()\n",
    "            tweetObj[ATTR_IS_SRC] = 1;\n",
    "            #FoundReply = FoundReply + 1\n",
    "        elif ID in SrcTestTweetsMap:\n",
    "            tweetObj = SrcTestTweetsMap[ID].copy()\n",
    "            tweetObj[ATTR_IS_SRC] = 0;\n",
    "    else:\n",
    "        if ID in ReplyTweetsMap:\n",
    "            tweetObj = ReplyTweetsMap[ID].copy()\n",
    "            tweetObj[ATTR_IS_SRC] = 1;\n",
    "            #FoundReply = FoundReply + 1\n",
    "        elif ID in SrcTweetsMap:\n",
    "            tweetObj = SrcTweetsMap[ID].copy()\n",
    "            tweetObj[ATTR_IS_SRC] = 0;\n",
    "            #FoundSrc = FoundSrc + 1\n",
    "        #else:\n",
    "            #NotFound = NotFound + 1\n",
    "            #print(\"tweet not found: \" + key)\n",
    "            \n",
    "    return tweetObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tweets structure\n",
    "import ntpath\n",
    "def GetStructs(rootPath,StructFileName):\n",
    "    StructJSON = { }\n",
    "    for root, subFolders, files in os.walk(rootPath):\n",
    "        for file in files:\n",
    "            if file == StructFileName:\n",
    "                tweetID = ntpath.basename(root)\n",
    "                StructFilePath = os.path.join(root,file)\n",
    "                with open(StructFilePath) as data_file:\n",
    "                    data = json.load(data_file)\n",
    "                    StructJSON[tweetID] = data\n",
    "    return StructJSON\n",
    "\n",
    "StructsJSON = GetStructs(TweetsRootPath,\"structure.json\")\n",
    "TestStructsJSON = GetStructs(TestTweetsRootPath,\"structure.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get Dict of source tweets for each tweet\n",
    "#print(type(StructsJSON[\"529653029747064832\"]))\n",
    "#print(StructsJSON[\"529653029747064832\"])\n",
    "\n",
    "#Dict of tweet and the value is the source tweet\n",
    "def ExtractSrcTweetsRecursive(SrcTweetKey,CurrentDict):\n",
    "    ResultDict = {}\n",
    "    if type(CurrentDict) is dict:\n",
    "        for key,val in CurrentDict.items(): \n",
    "            ResultDict[key] = SrcTweetKey\n",
    "            ResultDict.update(ExtractSrcTweetsRecursive(SrcTweetKey,val))\n",
    "    return ResultDict\n",
    "\n",
    "SrcTweetsDict = {}\n",
    "for key,val in StructsJSON.items():\n",
    "    SrcTweetsDict.update(ExtractSrcTweetsRecursive(key,val))\n",
    "    SrcTweetsDict[key] = key\n",
    "\n",
    "SrcTestTweetsDict = {}\n",
    "for key,val in TestStructsJSON.items():\n",
    "    SrcTestTweetsDict.update(ExtractSrcTweetsRecursive(key,val))\n",
    "    SrcTestTweetsDict[key] = key\n",
    "\n",
    "#print(len(SrcTweetsDict))\n",
    "#print(len(SrcTestTweetsDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting python files used\n",
    "%run QuestionDetector.py\n",
    "\n",
    "#Preparing utility objects\n",
    "questionDetector = QuestionDetector()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "StopWords = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "StopWords.sort()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import VectorizerMixin\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "from sklearn import cross_validation\n",
    "#from sklearn import  model_selection\n",
    "\n",
    "from sklearn.metrics import fbeta_score,make_scorer,accuracy_score\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import time\n",
    "from time import gmtime, strftime,strptime\n",
    "from datetime import datetime\n",
    "TWITTER_DATE_FORMAT = '%a %b %d %H:%M:%S +0000 %Y'\n",
    "MY_DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "#Cosine similarity to be used\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemmer\n",
    "\n",
    "def build_tokenizer():\n",
    "    \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\"\n",
    "    token_pattern = re.compile(token_pattern)\n",
    "    return lambda doc: token_pattern.findall(doc)\n",
    "\n",
    "\n",
    "def _word_ngrams(tokens, stop_words=None):\n",
    "    ngram_range = (1,2)\n",
    "    \"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\n",
    "    # handle stop words\n",
    "    if stop_words is not None:\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    # handle token n-grams\n",
    "    min_n, max_n = ngram_range\n",
    "    if max_n != 1:\n",
    "        original_tokens = tokens\n",
    "        tokens = []\n",
    "        n_original_tokens = len(original_tokens)\n",
    "        for n in range(min_n,\n",
    "                        min(max_n + 1, n_original_tokens + 1)):\n",
    "            for i in range(n_original_tokens - n + 1):\n",
    "                tokens.append(\" \".join(original_tokens[i: i + n]))\n",
    "\n",
    "    return tokens\n",
    "    \n",
    "#%run MyStemmer.py\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.stem import SnowballStemmer\n",
    "\n",
    "#stemmer = MyStemmer()\n",
    "stemmer = PorterStemmer()\n",
    "#stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def simpleStem(word):\n",
    "    \n",
    "    l = len(word)\n",
    "    if l > 3:\n",
    "        if word.endswith('es'):\n",
    "            print('before word = ' + word)\n",
    "            word = word[0:l-2]\n",
    "            print('after word = ' + word)\n",
    "        elif word.endswith('s'):\n",
    "            print('before word = ' + word)\n",
    "            word = word[0:l-1]\n",
    "            print('after word = ' + word)\n",
    "    return word\n",
    "        \n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        #stem= item\n",
    "        stem = stemmer.stem(item)\n",
    "        #stem = simpleStem(item)\n",
    "        \n",
    "        stemmed.append(stem)\n",
    "    return stemmed\n",
    "\n",
    "def tokenize_stem(text):\n",
    "    tokenize = build_tokenizer()\n",
    "    \n",
    "    tokens = _word_ngrams(tokenize(text), StopWords)\n",
    "    #tokens = nltk.word_tokenize(text)\n",
    "    #tokens = tknzr.tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer) \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract pos & neg words\n",
    "PosLexicon = opinion_lexicon.positive()\n",
    "NegLexicon = opinion_lexicon.negative()\n",
    "\n",
    "type(PosLexicon)\n",
    "\n",
    "PosLexiconList = []\n",
    "NegLexiconList = []\n",
    "for word in PosLexicon:\n",
    "    PosLexiconList.append(word)\n",
    "for word in NegLexicon:\n",
    "    NegLexiconList.append(word)\n",
    "#print(len(PosLexicon))\n",
    "#print(len(NegLexiconList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sentiment Analyzer using movie reviews corprus\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "class SentimentAnalyzer():\n",
    "\n",
    "    def word_feats(self,words):\n",
    "        return dict([(word, True) for word in words])\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        negids = movie_reviews.fileids('neg')\n",
    "        posids = movie_reviews.fileids('pos')\n",
    "\n",
    "        negfeats = [(self.word_feats(movie_reviews.words(fileids=[f])), 0) for f in negids]\n",
    "        posfeats = [(self.word_feats(movie_reviews.words(fileids=[f])), 1) for f in posids]\n",
    "\n",
    "        trainfeats = negfeats + posfeats\n",
    "\n",
    "        self.classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "#         print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "#         classifier.show_most_informative_features()\n",
    "        \n",
    "    def classify(self,sent):\n",
    "        \n",
    "        feat = self.word_feats(tknzr.tokenize(sent))\n",
    "        \n",
    "        return self.classifier.classify(feat)\n",
    "        \n",
    "SentAnalyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fooling around Using NLTK\n",
    "\n",
    "# import nltk\n",
    "# #nltk.download('punkt') #Used for tokenize\n",
    "# #nltk.download('stopwords')\n",
    "# #nltk.download()\n",
    "\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tknzr = TweetTokenizer()\n",
    "\n",
    "# tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "\n",
    "# tokens = tknzr.tokenize(tweet)\n",
    "# print(tokens)\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# import string\n",
    " \n",
    "# punctuation = list(string.punctuation)\n",
    "# stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "# terms_stop = [term for term in tokens if term not in stop]\n",
    "# print(terms_stop)\n",
    "\n",
    "# from nltk import bigrams\n",
    "# terms_bigram = list(bigrams(terms_stop))\n",
    "# print(terms_bigram)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
